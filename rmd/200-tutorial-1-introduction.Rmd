# Tutorial 1: Introduction

```{r, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning=FALSE)
options(knitr.kable.NA = '')

unloadNamespace("scexample")
unloadNamespace("sc")
library(data.table)
library(magrittr)
library(sc)

library(scexample)
```

## Setup

Implementing Sykdomspulsen Core requires a number of functions to be called in the correct order. To make this as simple as possible, we have provided a skeleton implementation at [sykdomspulsen-org/scskeleton](https://github.com/sykdomspulsen-org/scskeleton)

You should clone this [GitHub repo](https://github.com/sykdomspulsen-org/scskeleton) to your server. This will be the package that you will be working on throughout this tutorial. You may choose to do a global find/replace on `scskeleton` with the name you want for your R package. We will refer to this R package as your "sc implementation".

<!-- You should also clone [sykdomspulsen-org/scexample](https://github.com/sykdomspulsen-org/scexample) to your server. This is the end product of the tutorial, and you should refer to it in order to check your work. -->

For the purposes of this tutorial, we assume that the reader is either using RStudio Server Open Source or RStudio Workbench inside Docker containers that have been built according to the Sykdomspulsen specifications. We will refer to your implementation of RStudio Server Open Source/RStudio Workbench with the generic term "RStudio".

## Load the code

Open `scskeleton` in [RStudio project mode](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects). Restart the R session via `Ctrl+Shift+F10`, `rstudioapi::restartSession()`, or `Session > Restart R`. This will ensure that you have a clean working environment before you begin. You may now load your sc implementation. This can be done via `Ctrl+Shift+L`, `devtools::load_all(".")`, or `Build > Load All`.

<aside>
In general, we recommend cleaning your working environment every time before running `devtools::load_all(".")`.
</aside>

```{r eval=FALSE, include=TRUE}
rstudioapi::restartSession()
devtools::load_all(".")
```
It you are wokring in the sykdomspulsen infrastructure you might see a warning message on the form: sh: `1: /bin/authenticate.sh: not found`. This has to do with authentication beeing done automatically on sign in. You do not have to worry about this for the purpose of this tutorial.


You can now see which schemas have been loaded by running `sc::tm_get_schema_names()`. These schemas were included in the skeleton. Note that schemas beginning with `config_*` are special schemas that are automatically generated by `sc`. 

```{r, include=TRUE}
sc::tm_get_schema_names()
```

When you do this you will not see the schemas related to income and houseprices.

You can also see which tasks have been loaded by running `sc::tm_get_task_names()`. These tasks were included in the skeleton. Again you will not see the schemas related to income and houseprices.

```{r, include=TRUE}
sc::tm_get_task_names()
```

## Running

You can now run these tasks in your console if you want. Note that we use `scskeleton::tm_run_task` instead of `sc::tm_run_task`. This is because we want to ensure that `scexample::.onLoad` has been called which authenticates you.

```{r eval=FALSE, include=TRUE}
scskeleton::tm_run_task("weather_download_and_import_rawdata")
scskeleton::tm_run_task("weather_clean_data")
scskeleton::tm_run_task("weather_export_weather_plots")
```


The rest of the tutorial will walk you through how to create these three tasks.

## Weather data example

We are now going to recreate the weather data example. Our end goal is to plot the minimum and maximal temperature of all counties in Norway. This involves a task for downloading and importing raw data. For this we need to specify a schema which describes the data we want to store, which data identifies unique rows and who has access to the data. We also need to define the task through a task definition, i.e., task name, how many cores we want to use, the structure of the task, common arguments etc. Finally we actually implement the task by writing a data selector function, an action function and sometimes a more detailed function describing the plans and analyses of the task. 

We also create a task for cleaning the raw data, again with a schema, a task definition and an implementation of a data selector function and an action function.

Finally we create a task for the creation of the plots.

All the schemas are spesified in `03_db_schemas.r`, all the task definitions are specified in `04_tasks.r`. The data selector functions and the action functions corresponding to each task have there own respective script with the name specified in the task description.

## Developing weather_download_and_import_rawdata

We will walk you through the development of a task that downloads weather data from an API and imports the raw data into a database table.

### 1. Schemas

The first step when developing any task is specifying the schemas that will be used.

It is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("db-schemas/addins.png")
```

If you go into the script `03_db_schemas.r` you can see that all the schemas are within a function called `set_db_schemas` and that there is already a schema called `anon_example_weather_rawdata`. 
```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, lines = 18:64, include_url = TRUE)
```

We are now going to recreate this schema. 
Copy out the schemas that are already made (ctrl + shift + c).
Go to the end of the scrips but make sure your pointer is inside of the last curly bracket. Go to the `Addins` menu and click `Insert db schema (anon)`. You have now created a boiler plate for your schema. 

#### Schema name

Start by replacing `GROUPING_VARIANT` in `anon_GROUPING_VARIANT` with the name of your schema. For example `example_weather_rawdata`. The grouping will now be `example_weather` and the variant is `rawdata`.


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 20:22, include_url = TRUE)
```
Fill this inn for `name_grouping` and `name_variant`. The name of the schema is then `anon_example_weather_rawdata`.

In the example we define the name of the schema to be `anon_example_weather_weather_rawdata`.

#### Validators

The validators are pre-made and you do not have to change anything. 

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 61:63, include_url = TRUE)
```

These are validators that check:

- Are the column names/field types in the schema definition in line with style guidelines?
- Are the values/field contents of the datasets that will be uploaded to the database correct? E.g. Does a date column actually contain dates?

When using `validator_field_types = sc::validator_field_types_sykdomspulsen` we expect that the first 16 columns are always as follows (i.e. standardized structural data):

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 25:43, include_url = TRUE)
```

The field `info` should contain a short description of the data table. 

#### Field types/column names

Add the spesific column names and types needed. In this case add "temp_max", "temp_min", and "precip". These are all "DOUBLE". Also remove   `"XXXX_n" = "INTEGER"`, and `"XXXX_pr" = "DOUBLE"`.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 45:47, include_url = TRUE)
```

These are the extra columns that contain the specific data in this dataset.

#### Keys

The combination of these columns represents a unique row in the dataset. In this dataset the combination of "granularity_geo", "location_code", "date", "age", "sex" which are the initial suggestions are sufficient. 


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 49:55, include_url = TRUE)
```



#### Censoring

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 56:60, include_url = TRUE)
```

Censoring that is applied to the datasets. In this example we do not apply censoring hence remove the boiler plate suggestions. 

### 2. Task definition (task_from_config)

Now we have a schema. The second step is defining the task.

Again it is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

Go to script `04_tasks.r`. You can see that all task are already implemented. However, lets try and replicate them! Start by commenting out all the tasks already made (ctrs + shift + c). Go to the bottom of the script and place the pointer on the inside of the last curly bracket. Use the addins menu and click `Insert task_from_config`.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("tasks/addins_2.png")
```

Now you have a boilerplate for a task definition. 


```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 21:43, include_url = TRUE)
```

#### Task name

Replace `TASK_NAME` by your task name. For example `weather_download_and_import_rawdata`. `weather` is the task/grouping and `download_and_import_rawdata`is the action name. Insert these for `name_grouping`, and `name_action`. For `name_variant` use `NULL`.


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 22:24, include_url = TRUE)
```

Now the name of the task is defined to be `weather_download_and_import_rawdata`.

#### CPU cores

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 25, include_url = TRUE)
```

We specify that the plans will run sequentially with 1 CPU core. If the number of CPU cores is 2 or higher then the first and last plans will run sequentially, and all the plans in the middle will run in parallel. The first and last plans always run sequentially because this allows us to write "special" code for the first and last plans (i.e. "do this before everything runs" and "do this after everything runs").

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26:30, include_url = TRUE)
```

We specify the plan/analysis structure here. You may use one of the following combinations:

- `plan_analysis_fn_name` (rarely used)
- `for_each_plan` (plan-heavy, one analysis per plan)
- `for_each_plan` + `for_each_analysis` (typically analysis-heavy)

`plan_analysis_fn_name` is a (rarely used) function that will provide a list containing the plan/analysis structure. It is generally only used when the plan/analysis structure needs to be reactive depending upon some external data (e.g. "an unknown number of data files are provided each day and need to be cleaned").

`for_each_plan` is a list, with each element corresponding to a plan defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective plans. This particular `for_each_plan` defines a task with 356 plans (one for each municipality).

`for_each_analysis` is nearly the same as `for_each_plan`. It specifies what kind of analyses you would like to perform within each plan. It is a named list, with each element corresponding to an analysis defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective analyses.

An example of a `for_each_plan` that would correspond to 11 tasks (one for each county):

```{r, echo=TRUE}
options(width = 150)
for_each_plan = plnr::expand_list(
  location_code = fhidata::norway_locations_names()[granularity_geo %in% c("county")]$location_code
)
for_each_plan
```

`fhidata::norway_locations_names()` gives us location codes in Norway (try and run if in your console). Implement a plan in your task which has the location codes off all municipalities (municip) in Norway. 


### Universal argset

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 31, include_url = TRUE)
```

Here we can specify a named list, where each of the named elements will be translated into argset elements that are available for all plans/analyses.

#### Upsert/insert at end of each plan

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 32:33, include_url = TRUE)
```

If you include a schema called `output`, then these options will let you upsert/insert the returned value from `action_fn_name` at the end of each plan. This is an important nuance, because when you write/develop your task, you can (typically) only write one function (`action_fn_name`) that is applied to all analyses. This means that if your `action_fn` wants to upsert/insert data to a schema, it (typically) will do this within *every* analysis. If you have an analysis-heavy task, then this will be a lot of frequent traffic to the databases, which may affect performance. By using these flags, you can restrict the upsert/insert to the end of the plan, which may increase performance.

#### action_fn_name

The action_fn_name specifies the name of the function that corresponds to the action. That is, the function that is called in every analysis. Note that:

- This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_action`

In our case the package is `scskeleton` and the TASK is `weather_download_and_import_rawdata`. Insert this in your task.  

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 34, include_url = TRUE)
```


#### data_selector_fn_name

The data_selecotr_fn_name specifies the name of the function that corresponds to the data selector. That is, the function that is called at the start of every plan to provide data to all of the analyses inside the plan. Note that:

- This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_data_selector`

Try and guess what this would be in our example. 

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 35, include_url = TRUE)
```


#### Schemas

The schemas specify a named list, where each element consists of a schema. The names will be passed through as `schema$name` in `action_fn_name` and `data_selector_fn_name`. We must include both the schemas where we get data from and the schemas we store data to. 

In our example we do not yet have data so we only specify the schema we have earlier which we called `anon_example_weather_rawdata`. This meand you can remove the boiler plate input schema and replace `SCHEMA_NAME_2` with `anon_example_weather_rawdata`.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 36:41, include_url = TRUE)
```

#### Task description

Finally create a small task description. 

### 3. data_selector_fn

The third step in creating a task is defining a data selector function. This is the function that will perform the "one data-pull per plan" and subsequently provide the data to the action.

Go to script `weather_download_and_import_rawdata.r` and comment out everything. 

Use the RStudio `Addins` menu to help you quickly insert code templates by clicking `Insert action and data selector`.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("tasks/addins_3.png")
```


Just like that, a pre-made boilerplate is ready to go! Find the data_selector part of the script and replace `TASK_NAME` with our task name `weather_download_and_import_rawdata`. 


```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_download_and_import_rawdata.r")
print(x, lines = 88:119, include_url = TRUE)
```


#### plnr::is_run_directly()

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 94:101, include_url = TRUE)
```

At the top of all `data_selector_fn`s you will see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This code will **only** be run if it is manually highlighted inside RStudio and then "run". This is extremely beneficial to the user, because it means that the user can easily write small pieces of code that are only used during development, which will not be run when the code is run "properly".

Sykdomspulsen core uses these sections to let the user "jump" directly into the function. Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `argset` and `schema`.

The code inside `if (plnr::is_run_directly()) {` loads `argset` and `schema` for `index_plan = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_data_selector` as an interactive script!**

This makes the development of the code extremely easy as "everything is an interactive script".

Check that you have an argset and a schema by running the lines within `if (plnr::is_run_directly()) {}`.

### Getting data

The majority of the `data_selector_fn` is concerned with selecting data (obviously). Remember that the data should be selected to meet the needs of the plan. If you have 11 plans (one for each county), then your `data_selector_fn` should only extract data for the county of interest.


Take a look at your argset for plan = 1. 
Since we do not have input data from a schema we can remove the premade schema. Instead we are going to get data from `fhimaps::norway_lau2_map_b2020_default_dt` which provides latitudes (lat) and longitudes (long). Explore the available data by running `fhimaps::norway_lau2_map_b2020_default_dt` in your console. It returns a data table. We only want the mean latitude and longitude of the specific location_code for this particular plan and analysis. Therefor try and select only this data and call it `gps`. 

Now we download the weather forcast for this specific location from an api by using httr::GET and glue::glue to get the right address. 

httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/classic?lat={gps$lat}&lon={gps$long}"), httr::content_type_xml()). 
Use xml2::read_xml() to read the content. Take a peak below and implement it yourself.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 103:111, include_url = TRUE)
```


### Returning data

`data_selector_fn` needs to return a named list. This will be made available to the user in `action_fn` (`weather_download_and_import_rawdata_action`) via the argument `data`.

In your task replace "NAME" by the name for your data for example "data".

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 113:116, include_url = TRUE)
```

Check that the data selector function works by restarting R and loading the packages before running through the data selector function line by line. 

### 4. action_fn

The fourth step is defining an action function. This is the function that will perform the "action" within the the analysis. That is, given:

- data
- argset
- schema

What do you actually want to *do* with them? Find the action part in your script and replace  `TASK_NAME` with our task name `weather_download_and_import_rawdata`. 

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:86, include_url = TRUE)
```

#### plnr::is_run_directly()

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 95:102, include_url = TRUE)
```

At the top of all `action_fn`s you will again see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This works exactly the same as for the data_selector_fn. 

Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `data`, `argset` and `schema`. The code inside `if (plnr::is_run_directly()) {` loads `data`, `argset` and `schema` for `index_plan = 1` and `index_analysis = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_action` as an interactive script!**

Check out the data, argset and schema you have by running these lines.

#### argset$first_analysis

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 21:24, include_url = TRUE)
```

This code is only run if it is the first analysis. It is typically used to drop rows in a database, so that the following code may `insert` data (faster) instead of using `upsert` data (slower). If you ran the full task at the beginning of this tutorial you can insert `schema$anon_example_weather_rawdata$drop_all_rows()` inside here to delete the stored data.

#### Doing things

In this tutorial we do not go in to much detail about how the data is collected so for now copy the content of the action function from the task already implemented.
```{r, echo=TRUE, eval = FALSE}
    a <- data$data

    baz <- xml2::xml_find_all(a, ".//maxTemperature")
    res <- vector("list", length = length(baz))
    for (i in seq_along(baz)) {
      parent <- xml2::xml_parent(baz[[i]])
      grandparent <- xml2::xml_parent(parent)
      time_from <- xml2::xml_attr(grandparent, "from")
      time_to <- xml2::xml_attr(grandparent, "to")
      x <- xml2::xml_find_all(parent, ".//minTemperature")
      temp_min <- xml2::xml_attr(x, "value")
      x <- xml2::xml_find_all(parent, ".//maxTemperature")
      temp_max <- xml2::xml_attr(x, "value")
      x <- xml2::xml_find_all(parent, ".//precipitation")
      precip <- xml2::xml_attr(x, "value")
      res[[i]] <- data.frame(
        time_from = as.character(time_from),
        time_to = as.character(time_to),
        temp_max = as.numeric(temp_max),
        temp_min = as.numeric(temp_min),
        precip = as.numeric(precip)
      )
    }
    res <- rbindlist(res)
    res <- res[stringr::str_sub(time_from, 12, 13) %in% c("00", "06", "12", "18")]
    res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
    res[, N := .N, by = date]
    res <- res[N == 4]
    res <- res[
      ,
      .(
        temp_max = max(temp_max),
        temp_min = min(temp_min),
        precip = sum(precip)
      ),
      keyby = .(date)
    ]

    # we look at the downloaded data
     res

    # we now need to format it
    res[, granularity_time := "day"]
    res[, sex := "total"]
    res[, age := "total"]
    res[, location_code := argset$location_code]

    # fill in missing structural variables
    sc::fill_in_missing_v8(res, border = 2020)

    # we look at the downloaded data
     res

    # put data in db table
    schema$anon_example_weather_rawdata$insert_data(res)
```
   
   
    
```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26:80, include_url = TRUE)
```

Every analysis will perform this code.

Run through it line by line and pay special attention to how the data from the data_selector_fn is accesed, the last part where the data is formatted and we use `sc::fill_in_missing_v8(res, border = 2020)` to fill inn the mandatory data columns and the end where the data is inserted to the database. 

#### Accessing data from data_selector_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26, include_url = TRUE)
```

Here you see that we access the data that was passed to us from `data_selector_fn`

#### Structural data/sc::fill_in_missing_v8

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 68:74, include_url = TRUE)
```

We have 16 structural data columns that we expect. These columns typically have a lot of redundancy (e.g. date, isoyear, isoyearweek). To make things easier, we provide a function called `sc::fill_in_missing_v8` that uses the information present in the dataset to try and impute the missing structural data.

#### Insert/upsert to databases

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 80, include_url = TRUE)
```

Here we insert the data to the database table.

Insert is an append (so the data cannot already exist in the database table), while upsert is "update (overwrite) if already exists, insert (append) if it doesn't".

If you want to deleate the data use `schema$NAME_DATABASE$drop_all_rows()` in our case `schema$anon_example_weather_rawdata$drop_all_rows()`. 

#### argset$last_analysis

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 21:24, include_url = TRUE)
```

This code is only run if it is the last analysis. It is typically used to copy an internal database table (i.e. one that the public is not directly viewing) to an external database (i.e. one that the public is directly viewing).  

By distinguishing between internal database tables (e.g. anon_webkhtint_test) and external database tables (e.g. anon_webkht_test) we can do whatever we want to anon_webkhtint_test while anon_webkht_test remains in place and untouched. This makes it less likely that any mistakes will affect any APIs or websites that the public uses.


### Test the code
Try and restart, load all and run the code line by line. 

### Which plan/analysis is which?

Inside the `if (plnr::is_run_directly()) {` sections, you specify `index_plan` and `index_analysis`. However, these are just numbers. If you want to specifically look at the plan for Oslo municipality, how do you know which `index_plan` this corresponds to?

```{r, echo=TRUE}
options(width = 150)
sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
```

Try and change the plan number and run the script again.

Now you have implemented your first task by creating a schema, a task description a data selector function and an action function! Congratulations!

Run the entire task by running `tm_run_task("weather_download_and_import_rawdata")`.

## Developing weather_clean_data

The previous task (weather_download_and_import_rawdata) focused on downloading raw data from an API and inserting it into a database table.

The task weather_clean_data focuses on cleaning the raw data and inserting it in another database table. That is, the data source is a Sykdomspulsen Core database table, and the output is also a Sykdomspulsen Core database table.

We will walk you through the development of weather_clean_data, however, the description of this task will be less comprehensive than the previous task, and will focus primarily on parts that are novel.

We already mentioned that weather_clean_data cleans the raw data. We want this task to take the raw data we obtained in the previous task and aggregate it to obtain weather data on different geographical regions than municipalities. To do so we use some pre-made FHI functions such as `fhidata::make_skeleton` which makes a data table skeleton for the regions of interest and `fhidata::norway_locations_hierarchy` which converts location codes from one location code level to another. 

### 1. Schemas
First we start by creating a schema for the data we want this task to store. The structure is exactly the same as for the previous task with name access anon and temp_max, temp_min and precip as additional colums. Try and create this schema in `03_db_schemas.r`.

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, lines = 66:113, include_url = TRUE)
```

### 2. Task definition (task_from_config)

The next step is to define the task in `04_tasks.r` . For this task the aim is to aggregate data to higher levels meaning we need all data available at the same time and we preform the entire task in one analysis. Hence we need a task with only one plan (x = 1). We need the data from the database created in the previous task as input schema and the schema we just implemented as output schema. Try and create this task definition! (Remember to use the addins menu to get a boiler plate task definition.)


#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 52:56, include_url = TRUE)
```

For this particular task, we have decided to only implement one plan containing one analysis, which will process all of the data at once.

If we were only aggregating municipality data to the county level, we could have implemented 11 plans (one for each county). However, because we are also aggregating to the national level, we need all the data available at once.

#### Schemas

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 62:68, include_url = TRUE)
```

We need to specify the schemas that are used for both input and output.

#### Full task description

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 45:70, include_url = TRUE)
```

### 3. data_selector_fn

Now we are ready to create the data selector function. Go to script `weather_clean_data` and make sure everything is commented out. Use the addins menu as before to get a boiler plate for the action function and the data selector function and scroll down to the data selector part. Start by inserting your task name instead of `TASK_NAME`.

#### Getting data (specify the schema)

Next fill inn the name of the input schema instead of `SCHEMA_NAME`, connecting to the database table linked to the schema.

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_clean_data.r")
print(x, lines = 200, include_url = TRUE)
```


#### Getting data (sc::mandatory_db_filter)

We then introduce the sc::mandatory_db_filter. This is a filter on the most common structural variables. We say this is "mandatory" because we want the user to always keep in mind:

- The minimal amount of data needed to do the job
- To be as explicit as possible with what data is needed to do the job

Fill inn the mandatory filters as best you can and take a peak below if you are not sure.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 201:212, include_url = TRUE)
```

You will notice that we don't use all of the arguments passed into the function, but we use as many as we can.

#### Getting data (dplyr::select)
We always want to be as explicit as possible with what data is needed to do the job. To achieve this, we use `dplyr::select` to select the columns that we are interested in.

If you want to quickly generate a `dplyr::select` boilerplate for your schema that you can copy/paste, you can do this via either of the following:

```{r, echo=T, eval=FALSE}
schema$anon_example_weather_rawdata$print_dplyr_select()
```
```{r, echo=F, eval=T}
sc::config$schemas$anon_example_weather_rawdata$print_dplyr_select()
```

Use one of these functions and replace the dplyr::select part in your data selector function. To aggregate data we need location_code, date, temp_max, temp_min, precip, and the granularity_time (dayly, weekly, etc). Comment out the other variables. 


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 213:237, include_url = TRUE)
```


#### Getting data (dplyr::collect)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 238, include_url = TRUE)
```

This executes the SQL call to the database.

#### Getting data (data.table and setorder)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 239:243, include_url = TRUE)
```

Firstly, as a general rule we prefer to use data.table. So we would like to convert our data.frame to a data.table.

Secondly, we are not guaranteed to receive our data in any particular order. Because of this, it is very important that we sort our data on arrival (if this is relevant to the action_fn, e.g. if cumulative sums are created).

#### Set a name

Finally give the dataset you return a suitable name for example `day_municip`.

Check that you data selector function works by saving, restarting, and loading all packages. Then run through the function line by line. 

#### Example of the data_selector function
```{r, echo=FALSE}
options(width = 150)
print(x, lines = 184:251, include_url = TRUE)
```

### 4. action_fn

The final step in the process is creating the action function. Replace `TASK_NAME` with your task name.

#### Skeleton
In this action function we use fhi skeletons to create bases for our data tables. 
Read [here](https://folkehelseinstituttet.github.io/fhidata/articles/Skeletons.html) about the concept of skeletons.

Start by creating a variable (for example d_agg) for an empty list and copy the data collected in the data selector function into this (`d_agg$day_municip <- copy(data$day_municip)`). Extract the first and last date from this dataset.

Now we are going to create a skeleton for where we separate between regions where we have data (municipalities) and regions where we do not have data (bo og arbeigs regioner). The skeleton function takes min and max dates and in this case we will pass it granularity_geo consisting of a list with 


```{r, echo=TRUE}
list(
          "nodata" = c(
          "wardoslo",
          "extrawardoslo",
          "missingwardoslo",
          "wardbergen",
          "missingwardbergen",
          "wardstavanger",
          "missingwardstavanger",
          "notmainlandmunicip",
          "missingmunicip",
          "notmainlandcounty",
          "missingcounty"
        ),
        "municip" = c(
          "municip"
        )
)
```

#### Merge in weather data

Next we want to merge the information we have on weather data for the municipalities into this data. 


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 67:84, include_url = TRUE)
```

#### Aggregate to a county level 

Now aggregate the data to a county level with the help of `fhidata::norway_locations_hierarchy`

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 86:109, include_url = TRUE)
```


#### Aggregate to national level

There is no overlap in municipalities, hence aggregating to a national level can be done without the help of `fhidata::norway_locations_hierarchy`. 

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 111:127, include_url = TRUE)
```

#### Combine data

Combine all the different granularity geos by using rbindlist and storing it to a new name f.eks skeleton_day.



#### Weekly data. 

As a challenge try and aggregate the daily data to weekly data! You can use `fhiplot::isoyearweek_c(date)` to get the isoweek of a date. 
```{r, echo=FALSE}
options(width = 150)
print(x, lines = 134:153, include_url = TRUE)
```

#### Structural data

The next step is to fill in all missing structural data. Fill in sex = "total" and age= "total" manually then you can use `sc::fill_in_missing_v8(skeleton_day, border = config$border)`.
For the weekly data make sure to also convert the date by using `as.Date(date)` to ensure it is on the right format. 

Rbindlist binds the two data tables together.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 155:173, include_url = TRUE)
```

#### Store the data

Insert the final data table into the database specified in the task description

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 175:176, include_url = TRUE)
```


Restart R, load all packages and try and run the task. 

Run the entire task by running `tm_run_task("weather_clean_data")`. If you ran the tasks at the beginning of the script you might need to run `schema$anon_example_weather_data$drop_all_rows()` first. 

#### Full example 
```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:182, include_url = TRUE)
```


## Developing weather_export_plots

The final task of this tutorial, weather_export_plots, takes the cleaned data and plots 11 graphs (one for each county) of min and max temperatures.
This means we need 11 plans, one for each county. We use input data generated by data clean_weather_data. Hence, we do not need to create a new schema. We are going to pass a few universal argset through the task definition to define the location to store the figures. 


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 84:88, include_url = TRUE)
```

The benefits of placing the output directories and filenames in the task declaration are:

- It makes your action_fn more generic, and can be reused by multiple tasks
- It is easier to get an overview of where the output is being sent
- "More decisions" in the task config and "fewer decisions" in the action_fn makes the system easier for everyone to understand, because decisions become more explicit

Everything inside the curly brackets get passed through the action function. 

Each plan only need the data for that specific location_code. This can be implemented in the manditory filters in the data selector function. 

`fs::dir_create(glue::glue(argset$output_dir))` can be used to create the output directory. 

Try putting everything you have learned so fare together and create this task by yourself. If you get stuck you can always peak below. Good luck! 

### 1. Schemas

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, lines = 66:112, include_url = TRUE)
```

This schema has already been created by the previous task `weather_clean_data`.

### 2. Task definition (task_from_config)

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 72:100, include_url = TRUE)
```

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 79:83, include_url = TRUE)
```

Here we choose a plan-heavy approach (11 plans, 1 analysis per plan) to minimize the amount of data loaded into RAM at any point in time.

#### Universal argset

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 84:88, include_url = TRUE)
```


### 3. data_selector_fn

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_export_plots.r")
print(x, lines = 45:110, include_url = TRUE)
```

### 4. action_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:43, include_url = TRUE)
```

## What now?

After Tutorial 1, we expect that you understand the four fundamental parts of developing a task:

1. Schemas
2. Task definition (task_from_config)
3. data_selector_fn
4. action_fn

We also expect that you can:

1. Run a task using `tm_run_task`
2. Use `sc::tm_get_plans_argsets_as_dt` to identify which `index_plan` and `index_analysis` corresponds to the plan/analysis you are interested in (e.g. Oslo)
2. Run the inside code of a `data_selector_fn` for different `index_plan`s as if it were an interactive script
3. Run the inside code of an `action_fn` for different `index_plan`s and `index_analysis`s as if it were an interactive script

<!-- Tutorial 2 will challenge you to start creating your own tasks to solve problems. -->
