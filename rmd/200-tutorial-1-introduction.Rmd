# Tutorial 1: Introduction

```{r, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning=FALSE)
options(knitr.kable.NA = '')

unloadNamespace("scexample")
unloadNamespace("sc")
library(data.table)
library(magrittr)
library(sc)

library(scexample)
```

## Setup

Implementing Sykdomspulsen Core requires a number of functions to be called in the correct order. To make this as simple as possible, we have provided a skeleton implementation at [sykdomspulsen-org/scskeleton](https://github.com/sykdomspulsen-org/scskeleton)

You should clone this [GitHub repo](https://github.com/sykdomspulsen-org/scskeleton) to your server. This will be the package that you will be working on throughout this tutorial. You may choose to do a global find/replace on `scskeleton` with the name you want for your R package. We will refer to this R package as your "sc implementation".

You should also clone [sykdomspulsen-org/scexample](https://github.com/sykdomspulsen-org/scexample) to your server. This is the end product of the tutorial, and you should refer to it in order to check your work.

For the purposes of this tutorial, we assume that the reader is either using RStudio Server Open Source or RStudio Workbench inside Docker containers that have been built according to the Sykdomspulsen specifications. We will refer to your implementation of RStudio Server Open Source/RStudio Workbench with the generic term "RStudio".

## Load the code

Open `scskeleton` in [RStudio project mode](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects). Restart the R session via `Ctrl+Shift+F10`, `rstudioapi::restartSession()`, or `Session > Restart R`. This will ensure that you have a clean working environment before you begin. You may now load your sc implementation. This can be done via `Ctrl+Shift+L`, `devtools::load_all(".")`, or `Build > Load All`.

<aside>
In general, we recommend cleaning your working environment every time before running `devtools::load_all(".")`.
</aside>

```{r eval=FALSE, include=TRUE}
rstudioapi::restartSession()
devtools::load_all(".")
```
It you are wokring in the sykdomspulsen infrastructure you might see a warning message on the form: sh: `1: /bin/authenticate.sh: not found`. This has to do with authentication beeing done automatically on sign in. You do not have to worry about that for the purpose of this tutorial.


You can now see which schemas have been loaded by running `sc::tm_get_schema_names()`. These schemas were included in the skeleton. Note that schemas beginning with `config_*` are special schemas that are automatically generated by `sc`. 

```{r, include=TRUE}
sc::tm_get_schema_names()
```

When you do this you will not see the schemas related to income and houseprices.

You can also see which tasks have been loaded by running `sc::tm_get_task_names()`. These tasks were included in the skeleton. Again you will not see the schemas related to income and houseprices.

```{r, include=TRUE}
sc::tm_get_task_names()
```

## Running

You can now run these tasks. Note that we use `scskeleton::tm_run_task` instead of `sc::tm_run_task`. This is because we want to ensure that `scexample::.onLoad` has been called which authenticates you.

```{r eval=FALSE, include=TRUE}
scskeleton::tm_run_task("weather_download_and_import_rawdata")
scskeleton::tm_run_task("weather_clean_data")
scskeleton::tm_run_task("weather_export_weather_plots")
```


The rest of the tutorial will walk you through how to create these three tasks.

## Weather data example

We are now going to recreate the weather data example. Are goal is to plot the minimum and maximal temperature of a spesific reagion in Norway. This involves downloading and importing raw data. For this we need to specify a schema which describes the data we want to store, which data identify unique rows and who has access to the data. We also need to define the task generaly, i.e., task name, how many cores we want to use, the structure of the task, common arguments etc. Finally we actually implement the task by writing a data selector function, an action function and sometimes a more detailed function describing the plans and analyses of the task. 

We also create a task for cleaning the raw data, again with a schema, a task definition and an implementation of a data selector funciton and an action function.

Finally we create a task for the creation of some plots.

All the schemas are spesified in `03_db_schemas.r`, all the task definitions are spesified in `04_tasks.r`. The data selector functions and the actionsfunctions corresponding to each task have there own respecitve script with the name spesified in the task description.

## Developing weather_download_and_import_rawdata

We will walk you through the development of a task that downloads weather data from an API and imports the raw data into a database table.

### 1. Schemas

The first step when developing any task is specifying the schemas that will be used.

It is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("db-schemas/addins.png")
```

If you go into the script `03_db_schemas.r` you can see that all the schemas are within a function called `set_db_schemas` and that there is already a schema called `anon_example_weather_rawdata`. 
```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, lines = 18:64, include_url = TRUE)
```

We are now going to recreate this schema. 
Copy out the schemas that are alreaddy made (ctrl + shift + c).
Go to the end of the scrips but make sure your pointer is inside of the last curley bracket. Go to the `Addins` menu and click `Insert db schema (anon)`. You have now created a boiler plate for your schema. 

#### Schema name

Start by replacing `GROUPING_VARIANT` in `anon_GROUPING_VARIANT` with the name of your schema. For example `example_weather_rawdata`. The grouping will now be `example_weather` and the variant is `rawdata`.


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 20:22, include_url = TRUE)
```
Fill this inn for `name_grouping` and `name_variant`. The name of the schema is then `anon_example_weather_rawdata`.

In the example we define the name of the schema to be `anon_example_weather_weather_rawdata`.

#### Validators

The validators are premade and you do not have to change anything. 

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 61:63, include_url = TRUE)
```

These are validators that check:

- Are the column names/field types in the schema definition in line with style guidelines?
- Are the values/field contents of the datasets that will be uploaded to the database correct? E.g. Does a date column actually contain dates?

When using `validator_field_types = sc::validator_field_types_sykdomspulsen` we expect that the first 16 columns are always as follows (i.e. standardized structural data):

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 25:43, include_url = TRUE)
```

The field `info` should contain a short description of the data table. 

#### Field types/column names


Add the spesific column names and types needed. In this case add "temp_max", "temp_min", and "precip". These are all "DOUBLE". Also remove   `"XXXX_n" = "INTEGER"`, and `"XXXX_pr" = "DOUBLE"`.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 45:47, include_url = TRUE)
```

These are the extra columns that contain data.

#### Keys

The combination of these columns represents a unique row in the dataset. In this dataset the combination of "granularity_geo", "location_code", "date", "age", "sex" which are the initial suggestions are suffiecient. 


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 49:55, include_url = TRUE)
```



#### Censoring

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 56:60, include_url = TRUE)
```

Censoring that is applied to the datasets. In this example we do not apply censoring hence remove the boiler plate suggestions. 

### 2. Task definition (task_from_config)

Now we have a schema. The second step is defining the task.

Again it is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

Go to script `04_tasks.r`. You can see that all task are already implemented. However, lets try and replicate them! Start by commenting out all the tasks alreaddy made (ctrs + shift + c). Go to the bottom of the script and place the pointer on the inside of the last curley bracket. Use the addins menu and click `Insert task_from_config`.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("tasks/addins_2.png")
```

Now you have a boilerplate for a task. 


```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 21:43, include_url = TRUE)
```

#### Task name

Replase `TASK_NAME` by your task name. For example `weather_download_and_import_rawdata`. `weather` is the task/grouping and `download_and_import_rawdata`is the action name. Insert these for `name_grouping`, and `name_action`. For `name_variant` use `NULL`.


```{r, echo=FALSE}
options(width = 150)
print(x, lines = 22:24, include_url = TRUE)
```

Now the name of the task is defined to be `weather_download_and_import_rawdata`.

#### CPU cores

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 25, include_url = TRUE)
```

We specify that the plans will run sequentially with 1 CPU core. If the number of CPU cores is 2 or higher then the first and last plans will run sequentially, and all the plans in the middle will run in parallel. The first and last plans always run sequentially because this allows us to write "special" code for the first and last plans (i.e. "do this before everything runs" and "do this after everything runs").

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26:30, include_url = TRUE)
```

We specify the plan/analysis structure here. You may use one of the following combinations:

- `plan_analysis_fn_name` (rarely used)
- `for_each_plan` (plan-heavy, one analysis per plan)
- `for_each_plan` + `for_each_analysis` (typically analysis-heavy)

`plan_analysis_fn_name` is a (rarely used) function that will provide a list containing the plan/analysis structure. It is generally only used when the plan/analysis structure needs to be reactive depending upon some external data (e.g. "an unknown number of data files are provided each day and need to be cleaned").

`for_each_plan` is a list, with each element corresponding to a plan defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective plans. This particular `for_each_plan` defines a task with 356 plans (one for each municipality).

`for_each_analysis` is nearly the same as `for_each_plan`. It specifies what kind of analyses you would like to perform within each plan. It is a named list, with each element corresponding to an analysis defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective analyses.

An example of a `for_each_plan` that would correspond to 11 tasks (one for each county):

```{r, echo=TRUE}
options(width = 150)
for_each_plan = plnr::expand_list(
  location_code = fhidata::norway_locations_names()[granularity_geo %in% c("county")]$location_code
)
for_each_plan
```

`fhidata::norway_locations_names()` gives us location codes in Norway (try and run if in your console). Implement a plan in your task which has the location codes off all municipalities (municip) in Norway. 


### Universal argset

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 31, include_url = TRUE)
```

Here we can specify a named list, where each of the named elements will be translated into argset elements that are available for all plans/analyses.

#### Upsert/insert at end of each plan

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 32:33, include_url = TRUE)
```

If you include a schema called `output`, then these options will let you upsert/insert the returned value from `action_fn_name` at the end of each plan. This is an important nuance, because when you write/develop your task, you can (typically) only write one function (`action_fn_name`) that is applied to all analyses. This means that if your `action_fn` wants to upsert/insert data to a schema, it (typically) will do this within *every* analysis. If you have an analysis-heavy task, then this will be a lot of frequent traffic to the databases, which may affect performance. By using these flags, you can restrict the upsert/insert to the end of the plan, which may increase performance.

#### action_fn_name

The action_fn_name specifies the name of the function that corresponds to the action. That is, the function that is called in every analysis. Note that:

- This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_action`

In our case the package is `scskeleton` and the TASK is `weather_download_and_import_rawdata`. Insert this in your task.  

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 34, include_url = TRUE)
```


#### data_selector_fn_name

The data_selecotr_fn_name specifies the name of the function that corresponds to the data selector. That is, the function that is called at the start of every plan to provide data to all of the analyses inside the plan. Note that:

- This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_data_selector`

Try and guess what this would be in our example. 

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 35, include_url = TRUE)
```


#### Schemas

The schemas specify a named list, where each element consists of a schema. The names will be passed through as `schema$name` in `action_fn_name` and `data_selector_fn_name`. We must include both the schemas where we get data from and the schemas we store data to. 

In our example we do not yet have data so we only specify the schema we have earlier which we called `anon_example_weather_rawdata`. This meand you can remove the boiler plate input schema and replase `SCHEMA_NAME_2` with `anon_example_weather_rawdata`.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 36:41, include_url = TRUE)
```

#### Task description

Finally create a small task description. 

### 3. data_selector_fn

The third step in creating a task is defining a data selector function. This is the function that will perform the "one data-pull per plan" and subsequently provide the data to the action.

Go to script `weather_download_and_import_rawdata.r` and comment out everything. 

Use the RStudio `Addins` menu to help you quickly insert code templates by clicking `Insert action and data selector`.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("tasks/addins_3.png")
```


Just like that, a premade boilerplate is ready to go! Find the data_selector part of the script and replace `TASK_NAME` with our task name `weather_download_and_import_rawdata`. 


```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_download_and_import_rawdata.r")
print(x, lines = 88:119, include_url = TRUE)
```

#### plnr::is_run_directly()

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 94:101, include_url = TRUE)
```

At the top of all `data_selector_fn`s you will see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This code will **only** be run if it is manually highlighted inside RStudio and then "run". This is extremely beneficial to the user, because it means that the user can easily write small pieces of code that are only used during development, which will not be run when the code is run "properly".

Sykdomspulsen core uses these sections to let the user "jump" directly into the function. Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `argset` and `schema`.

The code inside `if (plnr::is_run_directly()) {` loads `argset` and `schema` for `index_plan = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_data_selector` as an interactive script!**

This makes the development of the code extremely easy as "everything is an interactive script".

### Getting data


The majority of the `data_selector_fn` is concerned with selecting data (obviously). Remember that the data should be selected to meet the needs of the plan. If you have 11 plans (one for each county), then your `data_selector_fn` should only extract data for the county of interest.


Take a look at your argset for plan = 1. 
Since we do not have input data from a schema we can remove the premade schema. Instead we are going to get data from `fhimaps::norway_lau2_map_b2020_default_dt` which provides latitudes (lat) and longitudes (long). Explore the availiable data by running `fhimaps::norway_lau2_map_b2020_default_dt` in your console. It returns a datatable and we only want the mean latitude and longitude of the specific location_code for this particular plan and analysis. Therefor try and select only this data and call it `gps`. 

Now we download the weatherforcast for this specific location from an api by using httr::GET and glue::glue to get teh right address. 

httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/classic?lat={gps$lat}&lon={gps$long}"), httr::content_type_xml()). 
Use xml2::read_xml() to read the content. Take a peak below and implement it yourself.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 103:111, include_url = TRUE)
```


### Returning data

`data_selector_fn` needs to return a named list. This will be made available to the user in `action_fn` (`weather_download_and_import_rawdata_action`) via the argument `data`.

In your task replace "NAME" by the name for your data for example "data".

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 113:116, include_url = TRUE)
```


### 4. action_fn

The fourth step is defining an action function. This is the function that will perform the "action" within the the analysis. That is, given:

- data
- argset
- schema

What do you actually want to *do* with them? Find the action part in your script and replace  `TASK_NAME` with our task name `weather_download_and_import_rawdata`. 

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:86, include_url = TRUE)
```

#### plnr::is_run_directly()

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 95:102, include_url = TRUE)
```

At the top of all `action_fn`s you will again see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This works exactly the same as for the data_selector_fn. 

Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `data`, `argset` and `schema`. The code inside `if (plnr::is_run_directly()) {` loads `data`, `argset` and `schema` for `index_plan = 1` and `index_analysis = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_action` as an interactive script!**


#### argset$first_analysis

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 21:24, include_url = TRUE)
```

This code is only run if it is the first analysis. It is typically used to drop rows in a database, so that the following code may `insert` data (faster) instead of using `upsert` data (slower).  

#### Doing things

In this tutorial we do not go in to much detail about how the data is collected so for now copy the content of the action function from the task already implemented.

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26:80, include_url = TRUE)
```

Every analysis will perform this code.

Run through it line by line and pay special attention to how the data from the data_selector_fn is accesed, the last part where the data is formatted and we use `sc::fill_in_missing_v8(res, border = 2020)` to fill inn the mandatory data columns and the end where the data is inserted to the database. 

#### Accessing data from data_selector_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26, include_url = TRUE)
```

Here you see that we access the data that was passed to us from `data_selector_fn`

#### Structural data/sc::fill_in_missing_v8

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 68:74, include_url = TRUE)
```

We have 16 structural data columns that we expect. These columns typically have a lot of redundancy (e.g. date, isoyear, isoyearweek). To make things easier, we provide a function called `sc::fill_in_missing_v8` that uses the information present in the dataset to try and impute the missing structural data.

#### Insert/upsert to databases

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 80, include_url = TRUE)
```

Here we insert the data to the database table.

Insert is an append (so the data cannot already exist in the database table), while upsert is "update (overwrite) if already exists, insert (append) if it doesn't".

If you want to deleate the data use `schema$NAME_DATABASE$drop_all_rows()` in our case `schema$anon_example_weather_rawdata$drop_all_rows()`. 

#### argset$last_analysis

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 21:24, include_url = TRUE)
```

This code is only run if it is the last analysis. It is typically used to copy an internal database table (i.e. one that the public is not directly viewing) to an external database (i.e. one that the public is directly viewing).  

By distinguishing between internal database tables (e.g. anon_webkhtint_test) and external database tables (e.g. anon_webkht_test) we can do whatever we want to anon_webkhtint_test while anon_webkht_test remains in place and untouched. This makes it less likely that any mistakes will affect any APIs or websites that the public uses.

### Which plan/analysis is which?

Inside the `if (plnr::is_run_directly()) {` sections, you specify `index_plan` and `index_analysis`. However, these are just numbers. If you want to specifically look at the plan for Oslo municipality, how do you know which `index_plan` this corresponds to?

```{r, echo=TRUE}
options(width = 150)
sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
```

Try and change the plan number and run the script again.

## Developing weather_clean_data

The previous task (weather_download_and_import_rawdata) focused on downloading raw data from an API and inserting it into a database table.

The task weather_clean_data focuses on cleaning the raw data and inserting it in another database table. That is, the data source is a Sykdomspulsen Core database table, and the output is also a Sykdomspulsen Core database table.

We will walk you through the development of weather_clean_data, however, the description of this task will be less comprehensive than the previous task, and will focus primarily on parts that are novel.


### 1. Schemas

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, include_url = TRUE)
```

### 2. Task definition (task_from_config)

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 45:70, include_url = TRUE)
```

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 52:56, include_url = TRUE)
```

For this particular task, we have decided to only implement one plan containing one analysis, which will process all of the data at once.

If we were only aggregating municipality data to the county level, we could have implemented 11 plans (one for each county). However, because we are also aggregating to the national level, we need all the data available at once.

#### Schemas

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 62:68, include_url = TRUE)
```

We need to specify the schemas that are used for both input and output.

### 3. data_selector_fn

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_clean_data.r")
print(x, lines = 184:251, include_url = TRUE)
```

#### Getting data (specify the schema)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 200, include_url = TRUE)
```

We start by connecting to the database table linked to the schema.

#### Getting data (sc::mandatory_db_filter)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 201:212, include_url = TRUE)
```

We then introduce the sc::mandatory_db_filter. This is a filter on the most common structural variables. We say this is "mandatory" because we want the user to always keep in mind:

- The minimal amount of data needed to do the job
- To be as explicit as possible with what data is needed to do the job

You will notice that we don't use all of the arguments passed into the function, but we use as many as we can.

#### Getting data (dplyr::select)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 213:237, include_url = TRUE)
```

We always want to be as explicit as possible with what data is needed to do the job. To achieve this, we use `dplyr::select` to select the columns that we are interested in.

If you want to quickly generate a `dplyr::select` boilerplate for your schema that you can copy/paste, you can do this via the following:

```{r, echo=T, eval=FALSE}
schema$anon_example_weather_rawdata$print_dplyr_select()
```
```{r, echo=F, eval=T}
sc::config$schemas$anon_example_weather_rawdata$print_dplyr_select()
```

#### Getting data (dplyr::collect)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 238, include_url = TRUE)
```

This executes the SQL call to the database.

#### Getting data (data.table and setorder)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 239:243, include_url = TRUE)
```

Firstly, as a general rule we prefer to use data.table. So we would like to convert our data.frame to a data.table.

Secondly, we are not guaranteed to receive our data in any particular order. Because of this, it is very important that we sort our data on arrival (if this is relevant to the action_fn, e.g. if cumulative sums are created).

### 4. action_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:182, include_url = TRUE)
```

#### Skeleton

Read [here](https://folkehelseinstituttet.github.io/fhidata/articles/Skeletons.html) about the concept of skeletons

## Developing weather_export_plots

The task weather_export_plots takes the cleaned data and plots 11 graphs (one for each county).

### 1. Schemas

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, lines = 66:112, include_url = TRUE)
```

This schema has already been created by the previous task `weather_clean_data`.

### 2. Task definition (task_from_config)

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 72:100, include_url = TRUE)
```

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 79:83, include_url = TRUE)
```

Here we choose a plan-heavy approach (11 plans, 1 analysis per plan) to minimize the amount of data loaded into RAM at any point in time.

#### Universal argset

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 84:88, include_url = TRUE)
```

The benefits of placing the output directories and filenames in the task declaration are:

- It makes your action_fn more generic, and can be reused by multiple tasks
- It is easier to get an overview of where the output is being sent
- "More decisions" in the task config and "fewer decisions" in the action_fn makes the system easier for everyone to understand, because decisions become more explicit

### 3. data_selector_fn

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_export_plots.r")
print(x, lines = 45:110, include_url = TRUE)
```

### 4. action_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:43, include_url = TRUE)
```

## What now?

After Tutorial 1, we expect that you understand the four fundamental parts of developing a task:

1. Schemas
2. Task definition (task_from_config)
3. data_selector_fn
4. action_fn

We also expect that you can:

1. Run a task using `tm_run_task`
2. Use `sc::tm_get_plans_argsets_as_dt` to identify which `index_plan` and `index_analysis` corresponds to the plan/analysis you are interested in (e.g. Oslo)
2. Run the inside code of a `data_selector_fn` for different `index_plan`s as if it were an interactive script
3. Run the inside code of an `action_fn` for different `index_plan`s and `index_analysis`s as if it were an interactive script

Tutorial 2 will challenge you to start creating your own tasks to solve problems.
