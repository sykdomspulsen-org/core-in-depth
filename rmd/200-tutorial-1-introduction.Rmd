# Tutorial 1: Introduction

```{r, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning=FALSE)
options(knitr.kable.NA = '')

unloadNamespace("scexample")
unloadNamespace("sc")
library(data.table)
library(magrittr)
library(sc)

library(scexample)
```

## Setup

Implementing Sykdomspulsen Core requires a number of functions to be called in the correct order. To make this as simple as possible, we have provided a skeleton implementation at [sykdomspulsen-org/scskeleton](https://github.com/sykdomspulsen-org/scskeleton)

You should clone this [GitHub repo](https://github.com/sykdomspulsen-org/scskeleton) to your server. This will be the package that you will be working on throughout this tutorial. You may choose to do a global find/replace on `scskeleton` with the name you want for your R package. We will refer to this R package as your "sc implementation".

You should also clone [sykdomspulsen-org/scexample](https://github.com/sykdomspulsen-org/scexample) to your server. This is the end product of the tutorial, and you should refer to it in order to check your work.

For the purposes of this tutorial, we assume that the reader is either using RStudio Server Open Source or RStudio Workbench inside Docker containers that have been built according to the Sykdomspulsen specifications. We will refer to your implementation of RStudio Server Open Source/RStudio Workbench with the generic term "RStudio".

## Load the code

Open `scskeleton` in [RStudio project mode](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects). Restart the R session via `Ctrl+Shift+F10`, `rstudioapi::restartSession()`, or `Session > Restart R`. This will ensure that you have a clean working environment before you begin. You may now load your sc implementation. This can be done via `Ctrl+Shift+L`, `devtools::load_all(".")`, or `Build > Load All`.

<aside>
In general, we recommend cleaning your working environment every time before running `devtools::load_all(".")`.
</aside>

```{r eval=FALSE, include=TRUE}
rstudioapi::restartSession()
devtools::load_all(".")
```

You can now see which schemas have been loaded. These schemas were included in the skeleton. Note that schemas beginning with `config_*` are special schemas that are automatically generated by `sc`.

```{r, include=TRUE}
sc::tm_get_schema_names()
```

You can now see which tasks have been loaded. These tasks were included in the skeleton.

```{r, include=TRUE}
sc::tm_get_task_names()
```

## Running

You can now run these tasks. Note that we use `scskeleton::tm_run_task` instead of `sc::tm_run_task`. This is because we want to ensure that `scexample::.onLoad` has been called.

```{r eval=FALSE, include=TRUE}
scskeleton::tm_run_task("weather_download_and_import_rawdata")
scskeleton::tm_run_task("weather_clean_data")
scskeleton::tm_run_task("weather_export_weather_plots")
```

## Developing weather_download_and_import_rawdata

We will walk you through the development of a task that downloads weather data from an API and imports the raw data into a database table.

### 1. Schemas

The first step when developing any task is specifying the schemas that will be used.

It is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("rmd/db-schemas/addins.png")
```

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, lines = 18:64, include_url = TRUE)
```

#### Schema name

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 20:22, include_url = TRUE)
```

Here we define the name of the schema to be `anon_example_weather_rawdata`.

#### Validators

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 61:63, include_url = TRUE)
```

These are validators that check:

- Are the column names/field types in the schema definition in line with style guidelines?
- Are the values/field contents of the datasets that will be uploaded to the database correct? E.g. Does a date column actually contain dates?

When using `validator_field_types = sc::validator_field_types_sykdomspulsen` we expect that the first 16 columns are always as follows (i.e. standardized structural data).

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 25:43, include_url = TRUE)
```

#### Field types/column names

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 45:47, include_url = TRUE)
```

The extra columns that contain data.

#### Keys

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 49:55, include_url = TRUE)
```

The combination of these columns represents a unique row in the dataset.

#### Censoring

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 56:60, include_url = TRUE)
```

Censoring that is applied to the datasets.

### 2. Task definition (task_from_config)

The second step is defining the task.

It is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("rmd/tasks/addins_2.png")
```

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 21:43, include_url = TRUE)
```

#### Task name

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 22:24, include_url = TRUE)
```

Here we define the name of the task to be `weather_download_and_import_rawdata`.

#### CPU cores

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 25, include_url = TRUE)
```

We specify that the plans will run sequentially with 1 CPU core. If the number of CPU cores is 2 or higher then the first and last plans will run sequentially, and all the plans in the middle will run in parallel. The first and last plans always run sequentially because this allows us to write "special" code for the first and last plans (i.e. "do this before everything runs" and "do this after everything runs").

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26:30, include_url = TRUE)
```

We specify the plan/analysis structure here. You may use one of the following combinations:

- `plan_analysis_fn_name` (rarely used)
- `for_each_plan` (plan-heavy, one analysis per plan)
- `for_each_plan` + `for_each_analysis` (typically analysis-heavy)

`plan_analysis_fn_name` is a (rarely used) function that will provide a list containing the plan/analysis structure. It is generally only used when the plan/analysis structure needs to be reactive depending upon some external data (e.g. "an unknown number of data files are provided each day and need to be cleaned").

`for_each_plan` is a list, with each element corresponding to a plan defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective plans. This particular `for_each_plan` defines a task with 356 plans (one for each municipality).

`for_each_analysis` is nearly the same as `for_each_plan`. It specifies what kind of analyses you would like to perform within each plan. It is a named list, with each element corresponding to an analysis defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective analyses.

An example of a `for_each_plan` that would correspond to 11 tasks (one for each county):

```{r, echo=TRUE}
options(width = 150)
for_each_plan = plnr::expand_list(
  location_code = fhidata::norway_locations_names()[granularity_geo %in% c("county")]$location_code
)
for_each_plan
```

#### Universal argset

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 31, include_url = TRUE)
```

Here we can specify a named list, where each of the named elements will be translated into argset elements that are available for all plans/analyses.

#### Upsert/insert at end of each plan

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 32:33, include_url = TRUE)
```

If you include a schema called `output`, then these options will let you upsert/insert the returned value from `action_fn_name` at the end of each plan. This is an important nuance, because when you write/develop your task, you can (typically) only write one function (`action_fn_name`) that is applied to all analyses. This means that if your `action_fn` wants to upsert/insert data to a schema, it (typically) will do this within *every* analysis. If you have an analysis-heavy task, then this will be a lot of frequent traffic to the databases, which may affect performance. By using these flags, you can restrict the upsert/insert to the end of the plan, which may increase performance.

#### action_fn_name

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 34, include_url = TRUE)
```

Here we specify the name of the function that corresponds to the action. That is, the function that is called in every analysis. Note that:

- This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_action`

#### data_selector_fn_name

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 35, include_url = TRUE)
```

Here we specify the name of the function that corresponds to the data selector. That is, the function that is called at the start of every plan to provide data to all of the analyses inside the plan. Note that:

- This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_data_selector`

#### Schemas

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 36:41, include_url = TRUE)
```

Here we specify a named list, where each element consists of a schema. The names will be passed through as `schema$name` in `action_fn_name` and `data_selector_fn_name`.

### 3. data_selector_fn

The third step is defining a data selector function. This is the function that will perform the "one data-pull per plan" and subsequently provide the data to the action.

It is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

```{r, out.width = "100%", echo = FALSE}
knitr::include_graphics("rmd/tasks/addins_3.png")
```

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_download_and_import_rawdata.r")
print(x, lines = 88:119, include_url = TRUE)
```

#### plnr::is_run_directly()

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 94:101, include_url = TRUE)
```

At the top of all `data_selector_fn`s you will see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This code will **only** be run if it is manually highlighted inside RStudio and then "run". This is extremely beneficial to the user, because it means that the user can easily write small pieces of code that are only used during development, which will not be run when the code is run "properly".

Sykdomspulsen core uses these sections to let the user "jump" directly into the function. Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `argset` and `schema`.

The code inside `if (plnr::is_run_directly()) {` loads `argset` and `schema` for `index_plan = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_data_selector` as an interactive script!**

This makes the development of the code extremely easy as "everything is an interactive script".

#### Getting data

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 103:111, include_url = TRUE)
```

The majority of the `data_selector_fn` is concerned with selecting data (obviously). Remember that the data should be selected to meet the needs of the plan. If you have 11 plans (one for each county), then your `data_selector_fn` should only extract data for the county of interest.

#### Returning data

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 113:116, include_url = TRUE)
```

`data_selector_fn` needs to return a named list. This will be made available to the user in `action_fn` (`weather_download_and_import_rawdata_action`) via the argument `data`.

### 4. action_fn

The fourth step is defining an action function. This is the function that will perform the "action" within the the analysis. That is, given:

- data
- argset
- schema

What do you actually want to *do* with them?

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:86, include_url = TRUE)
```

#### plnr::is_run_directly()

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 95:102, include_url = TRUE)
```

At the top of all `action_fn`s you will see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This code will **only** be run if it is manually highlighted inside RStudio and then "run". This is extremely beneficial to the user, because it means that the user can easily write small pieces of code that are only used during development, which will not be run when the code is run "properly".

Sykdomspulsen core uses these sections to let the user "jump" directly into the function. Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `data`, `argset` and `schema`. The code inside `if (plnr::is_run_directly()) {` loads `data`, `argset` and `schema` for `index_plan = 1` and `index_analysis = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_action` as an interactive script!**

This makes the development of the code extremely easy as "everything is an interactive script".

#### argset$first_analysis

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 21:24, include_url = TRUE)
```

This code is only run if it is the first analysis. It is typically used to drop rows in a database, so that the following code may `insert` data (faster) instead of using `upsert` data (slower).  

#### Doing things

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26:80, include_url = TRUE)
```

Every analysis will perform this code.

#### Accessing data from data_selector_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 26, include_url = TRUE)
```

Here you see that we access the data that was passed to us from `data_selector_fn`

#### Structural data/sc::fill_in_missing_v8

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 68:74, include_url = TRUE)
```

We have 16 structural data columns that we expect. These columns typically have a lot of redundancy (e.g. date, isoyear, isoyearweek). To make things easier, we provide a function called `sc::fill_in_missing_v8` that uses the information present in the dataset to try and impute the missing structural data.

#### Insert/upsert to databases

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 80, include_url = TRUE)
```

Here we insert the data to the database table.

Remember that insert is an append (so the data cannot already exist in the database table), while upsert is "update (overwrite) if already exists, insert (append) if it doesn't".

#### argset$last_analysis

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 21:24, include_url = TRUE)
```

This code is only run if it is the last analysis. It is typically used to copy an internal database table (i.e. one that the public is not directly viewing) to an external database (i.e. one that the public is directly viewing).  

By distinguishing between internal database tables (e.g. anon_webkhtint_test) and external database tables (e.g. anon_webkht_test) we can do whatever we want to anon_webkhtint_test while anon_webkht_test remains in place and untouched. This makes it less likely that any mistakes will affect any APIs or websites that the public uses.

### Which plan/analysis is which?

Inside the `if (plnr::is_run_directly()) {` sections, you specify `index_plan` and `index_analysis`. However, these are just numbers. If you want to specifically look at the plan for Oslo municipality, how do you know which `index_plan` this corresponds to?

```{r, echo=TRUE}
options(width = 150)
sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
```

## Developing weather_clean_data

The previous task (weather_download_and_import_rawdata) focused on downloading raw data from an API and inserting it into a database table.

The task weather_clean_data focuses on cleaning the raw data and inserting it in another database table. That is, the data source is a Sykdomspulsen Core database table, and the output is also a Sykdomspulsen Core database table.

We will walk you through the development of weather_clean_data, however, the description of this task will be less comprehensive than the previous task, and will focus primarily on parts that are novel.


### 1. Schemas

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, include_url = TRUE)
```

### 2. Task definition (task_from_config)

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 45:70, include_url = TRUE)
```

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 52:56, include_url = TRUE)
```

For this particular task, we have decided to only implement one plan containing one analysis, which will process all of the data at once.

If we were only aggregating municipality data to the county level, we could have implemented 11 plans (one for each county). However, because we are also aggregating to the national level, we need all the data available at once.

#### Schemas

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 62:68, include_url = TRUE)
```

We need to specify the schemas that are used for both input and output.

### 3. data_selector_fn

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_clean_data.r")
print(x, lines = 184:251, include_url = TRUE)
```

#### Getting data (specify the schema)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 200, include_url = TRUE)
```

We start by connecting to the database table linked to the schema.

#### Getting data (sc::mandatory_db_filter)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 201:212, include_url = TRUE)
```

We then introduce the sc::mandatory_db_filter. This is a filter on the most common structural variables. We say this is "mandatory" because we want the user to always keep in mind:

- The minimal amount of data needed to do the job
- To be as explicit as possible with what data is needed to do the job

You will notice that we don't use all of the arguments passed into the function, but we use as many as we can.

#### Getting data (dplyr::select)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 213:237, include_url = TRUE)
```

We always want to be as explicit as possible with what data is needed to do the job. To achieve this, we use `dplyr::select` to select the columns that we are interested in.

If you want to quickly generate a `dplyr::select` boilerplate for your schema that you can copy/paste, you can do this via the following:

```{r, echo=T, eval=FALSE}
schema$anon_example_weather_rawdata$print_dplyr_select()
```
```{r, echo=F, eval=T}
sc::config$schemas$anon_example_weather_rawdata$print_dplyr_select()
```

#### Getting data (dplyr::collect)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 238, include_url = TRUE)
```

This executes the SQL call to the database.

#### Getting data (data.table and setorder)

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 239:243, include_url = TRUE)
```

Firstly, as a general rule we prefer to use data.table. So we would like to convert our data.frame to a data.table.

Secondly, we are not guaranteed to receive our data in any particular order. Because of this, it is very important that we sort our data on arrival (if this is relevant to the action_fn, e.g. if cumulative sums are created).

### 4. action_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:182, include_url = TRUE)
```

#### Skeleton

Read [here](https://folkehelseinstituttet.github.io/fhidata/articles/Skeletons.html) about the concept of skeletons

## Developing weather_export_plots

The task weather_export_plots takes the cleaned data and plots 11 graphs (one for each county).

### 1. Schemas

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://raw.githubusercontent.com/sykdomspulsen-org/scskeleton/main/R/03_db_schemas.r")
print(x, lines = 66:112, include_url = TRUE)
```

This schema has already been created by the previous task `weather_clean_data`.

### 2. Task definition (task_from_config)

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/04_tasks.r")
print(x, lines = 72:100, include_url = TRUE)
```

#### Plan/analysis structure

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 79:83, include_url = TRUE)
```

Here we choose a plan-heavy approach (11 plans, 1 analysis per plan) to minimize the amount of data loaded into RAM at any point in time.

#### Universal argset

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 84:88, include_url = TRUE)
```

The benefits of placing the output directories and filenames in the task declaration are:

- It makes your action_fn more generic, and can be reused by multiple tasks
- It is easier to get an overview of where the output is being sent
- "More decisions" in the task config and "fewer decisions" in the action_fn makes the system easier for everyone to understand, because decisions become more explicit

### 3. data_selector_fn

```{r, echo=FALSE}
options(width = 150)
x <- fhiplot::as_github_code("https://github.com/sykdomspulsen-org/scskeleton/blob/main/R/weather_export_plots.r")
print(x, lines = 45:110, include_url = TRUE)
```

### 4. action_fn

```{r, echo=FALSE}
options(width = 150)
print(x, lines = 1:43, include_url = TRUE)
```

## What now?

After Tutorial 1, we expect that you understand the four fundamental parts of developing a task:

1. Schemas
2. Task definition (task_from_config)
3. data_selector_fn
4. action_fn

We also expect that you can:

1. Run a task using `tm_run_task`
2. Use `sc::tm_get_plans_argsets_as_dt` to identify which `index_plan` and `index_analysis` corresponds to the plan/analysis you are interested in (e.g. Oslo)
2. Run the inside code of a `data_selector_fn` for different `index_plan`s as if it were an interactive script
3. Run the inside code of an `action_fn` for different `index_plan`s and `index_analysis`s as if it were an interactive script

Tutorial 2 will challenge you to start creating your own tasks to solve problems.
